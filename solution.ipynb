{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan\n",
    "# Augment data\n",
    "# Use CNN with CGG16 architecture with removed FC as feature extractor\n",
    "# Apply classification(svm-c/log-c/lin-c)\n",
    "# ???\n",
    "# Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import transform, image\n",
    "from keras.applications import VGG16, imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array, load_img, array_to_img\n",
    "from sklearn import linear_model, svm, metrics\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'superbowllsh/train/'\n",
    "TEST_DIR = 'superbowllsh/test/'\n",
    "LABELS = ['cleaned', 'dirty']\n",
    "BATCH_SIZE = 16\n",
    "LE_PATH = os.path.sep.join([\"output\", \"le.cpickle\"])\n",
    "BASE_CSV_PATH = \"output\"\n",
    "# set the path to the serialized model after training\n",
    "MODEL_PATH = 'output/model.cpickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_torch_to_keras(array):\n",
    "    r = []\n",
    "    for layer in array:\n",
    "        layer = np.expand_dims(layer, 2)\n",
    "        if not len(r):\n",
    "            r = layer\n",
    "            continue\n",
    "        r = np.c_[r, layer]\n",
    "    return r\n",
    "\n",
    "def augment_image(path, size=(224, 224), x=10):\n",
    "    img = image.open_image(path)\n",
    "    augmented = [img.apply_tfms(tfms=tfms[0], size=size, padding_mode='border') for i in range(x)]\n",
    "    orig = load_img(path, target_size=size)\n",
    "    return [orig] + [array_to_img(aug_img.data, data_format='channels_first') for aug_img in augmented]\n",
    "\n",
    "def get_image_id(img_path, dataset_path):\n",
    "    return re.findall('{}([0-9]*).jpg'.format(dataset_path), img_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # load the VGG16 network and initialize the label encoder\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "    tfms = transform.get_transforms(do_flip=True, flip_vert=True, max_rotate=30., max_zoom=1.05)\n",
    "    train_x = np.array([])\n",
    "    train_y = np.array([])\n",
    "    # Collect all train images\n",
    "    train_imgs = np.array([])\n",
    "    for y, label in enumerate(LABELS):\n",
    "        img_paths = list(paths.list_images(TRAIN_DIR + label))\n",
    "        for path in img_paths:\n",
    "            # Augment original image 5x times + original image, all in size = (244, 244, 3)\n",
    "            augmented_imgs = np.array([img_to_array(img)/255 for img in augment_image(path, x=4)])\n",
    "            augmented_y = np.ones(augmented_imgs.shape[0]) * y\n",
    "            if train_imgs.shape == (0,):\n",
    "                train_imgs = augmented_imgs\n",
    "                train_y = augmented_y\n",
    "                continue\n",
    "            train_imgs = np.r_[train_imgs, augmented_imgs]\n",
    "            train_y = np.r_[train_y, augmented_y]\n",
    "    dataset_size = train_imgs.shape[0]\n",
    "    for start in range(0, dataset_size, BATCH_SIZE):\n",
    "        batchImages = train_imgs[start:np.min((start+BATCH_SIZE, dataset_size))]\n",
    "        features = model.predict(batchImages, batch_size=np.min((BATCH_SIZE, len(batchImages))))\n",
    "        features = features.reshape((features.shape[0], np.prod(features.shape[1:])))\n",
    "        if train_x.shape == (0,):\n",
    "            train_x = features\n",
    "            continue\n",
    "        train_x = np.r_[train_x, features]\n",
    "    return train_x, train_y\n",
    "\n",
    "def process_train(train_x, train_y):\n",
    "    train_xy = np.c_[train_x, train_y]\n",
    "    np.random.shuffle(train_xy)\n",
    "    \n",
    "    # train the model\n",
    "    model = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=(10**7))\n",
    "    model.fit(train_xy[:,:-1], train_xy[:,-1])\n",
    "\n",
    "    # serialize the model to disk\n",
    "    f = open(MODEL_PATH, \"wb\")\n",
    "    f.write(pickle.dumps(model))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_dir=TEST_DIR, predict_csv='test_prediction.csv'):\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "    img_paths = list(paths.list_images(TEST_DIR))\n",
    "    img_paths = sorted(img_paths, key=lambda path: int(get_image_id(path, dataset_dir)))\n",
    "    dataset_size = len(img_paths)\n",
    "    test_x = np.array([])\n",
    "    ids = []\n",
    "    for start in range(0, dataset_size, BATCH_SIZE):\n",
    "        batchImages = np.array([])\n",
    "        for path in img_paths[start:np.min((start + BATCH_SIZE, dataset_size))]:\n",
    "            id = get_image_id(path, dataset_dir)\n",
    "            ids += [id]\n",
    "            img = load_img(path, target_size=(224, 224))\n",
    "            img = img_to_array(img) / 255\n",
    "            if batchImages.shape == (0,):\n",
    "                batchImages = np.array([img])\n",
    "                continue\n",
    "            batchImages = np.r_[batchImages, np.array([img])]\n",
    "        features = model.predict(batchImages, batch_size=batchImages.shape[0])\n",
    "        features = features.reshape((features.shape[0], np.prod(features.shape[1:])))\n",
    "        if test_x.shape == (0,):\n",
    "            test_x = features\n",
    "            continue\n",
    "        test_x = np.r_[test_x, features]\n",
    "\n",
    "    # deserialize model of classifier\n",
    "    model = pickle.load(open(MODEL_PATH, 'rb'))\n",
    "\n",
    "    # evaluate the model\n",
    "    preds = model.predict(test_x)\n",
    "    df = pd.DataFrame(\n",
    "        np.array([[ids[i], LABELS[int(p)]] for i, p in enumerate(preds)]),\n",
    "        columns=('id', 'label')\n",
    "    )\n",
    "    df.to_csv(BASE_CSV_PATH + '/' + predict_csv, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test dataset labels\n",
    "# df = test()\n",
    "def test():\n",
    "    return predict()\n",
    "\n",
    "# Predict validate dataset labels\n",
    "# df = validate()\n",
    "def validate(dataset_dir='superbowllsh/validate/', predict_csv='validate_prediction.csv'):\n",
    "    predict(dataset_dir=dataset_dir, predict_csv=predict_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7(opencv4)",
   "language": "python",
   "name": "cv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
